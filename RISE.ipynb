{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface\n",
        "!pip install transformers[torch]\n",
        "!pip install datasets\n",
        "!pip install evaluate\n",
        "!pip install seqeval\n",
        "!pip install accelerate -U"
      ],
      "metadata": {
        "id": "vtpSPzTeQ_BG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import huggingface\n",
        "import huggingface_hub\n",
        "from datasets import load_dataset\n",
        "import datasets\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import DataCollatorForTokenClassification\n",
        "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
        "\n",
        "import evaluate"
      ],
      "metadata": {
        "id": "Oo4N0A69qk-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "username = \"HUGGINFACE_USERNAME\n",
        "model1 = \"RISE_NER\"\n",
        "model2 = \"RISE_NER_REDUCED\""
      ],
      "metadata": {
        "id": "Xl0-gEuGjYAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "key = \"HUGGINGFACE_KEY\"\n",
        "huggingface_hub.login(key)"
      ],
      "metadata": {
        "id": "IYfVcE4y7XzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = load_dataset(\"Babelscape/multinerd\", split=\"train\")\n",
        "valid_dataset = load_dataset(\"Babelscape/multinerd\", split=\"validation\")\n",
        "test_dataset  = load_dataset(\"Babelscape/multinerd\", split=\"test\")"
      ],
      "metadata": {
        "id": "7qpn6Y_LU31e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_en = train_dataset.filter(lambda x: x[\"lang\"] == 'en')\n",
        "valid_en = valid_dataset.filter(lambda x: x[\"lang\"] == 'en')\n",
        "test_en = test_dataset.filter(lambda x: x[\"lang\"] == 'en')"
      ],
      "metadata": {
        "id": "RdlF86oYYd2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_en[0]"
      ],
      "metadata": {
        "id": "4-jHMyj_0vAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label2id = {\n",
        "    \"O\": 0,\n",
        "    \"B-PER\": 1,\n",
        "    \"I-PER\": 2,\n",
        "    \"B-ORG\": 3,\n",
        "    \"I-ORG\": 4,\n",
        "    \"B-LOC\": 5,\n",
        "    \"I-LOC\": 6,\n",
        "    \"B-ANIM\": 7,\n",
        "    \"I-ANIM\": 8,\n",
        "    \"B-BIO\": 9,\n",
        "    \"I-BIO\": 10,\n",
        "    \"B-CEL\": 11,\n",
        "    \"I-CEL\": 12,\n",
        "    \"B-DIS\": 13,\n",
        "    \"I-DIS\": 14,\n",
        "    \"B-EVE\": 15,\n",
        "    \"I-EVE\": 16,\n",
        "    \"B-FOOD\": 17,\n",
        "    \"I-FOOD\": 18,\n",
        "    \"B-INST\": 19,\n",
        "    \"I-INST\": 20,\n",
        "    \"B-MEDIA\": 21,\n",
        "    \"I-MEDIA\": 22,\n",
        "    \"B-MYTH\": 23,\n",
        "    \"I-MYTH\": 24,\n",
        "    \"B-PLANT\": 25,\n",
        "    \"I-PLANT\": 26,\n",
        "    \"B-TIME\": 27,\n",
        "    \"I-TIME\": 28,\n",
        "    \"B-VEHI\": 29,\n",
        "    \"I-VEHI\": 30,\n",
        "  }\n",
        "id2label = {value: key for key, value in label2id.items()}\n",
        "\n",
        "wanted_tags = ['B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-ANIM', 'I-ANIM', 'B-DIS', 'I-DIS', \"O\"]"
      ],
      "metadata": {
        "id": "Oo7e_IJzgk_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_in(key):\n",
        "  \"\"\"Checks if given key is one of the 5 categories defined by the task\"\"\"\n",
        "  return id2label[key] in wanted_tags\n",
        "\n",
        "def test_is_in():\n",
        "  assert (is_in(1) == True)\n",
        "  assert (is_in(20) == False)\n",
        "  assert (is_in(2) == True)\n",
        "  assert (is_in(3) == True)\n",
        "  assert (is_in(4) == True)\n",
        "  assert (is_in(5) == True)\n",
        "  assert (is_in(6) == True)\n",
        "  assert (is_in(7) == True)\n",
        "  assert (is_in(8) == True)\n",
        "  assert (is_in(13) == True)\n",
        "  assert (is_in(14) == True)\n",
        "  print('Test complete. No Errors')\n",
        "\n",
        "test_is_in()\n",
        "\n",
        "def convert_to_five_categories(key):\n",
        "  \"\"\"Takes a tag and either keeps it as is or converts it to 0 if not one of the desired categories\"\"\"\n",
        "  if is_in(key):\n",
        "    return key\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "def test_converter():\n",
        "  assert (convert_to_five_categories(1) == 1)\n",
        "  assert (convert_to_five_categories(20) == 0)\n",
        "  assert (convert_to_five_categories(2) == 2)\n",
        "  assert (convert_to_five_categories(3) == 3)\n",
        "  assert (convert_to_five_categories(4) == 4)\n",
        "  assert (convert_to_five_categories(5) == 5)\n",
        "  assert (convert_to_five_categories(6) == 6)\n",
        "  assert (convert_to_five_categories(7) == 7)\n",
        "  assert (convert_to_five_categories(8) == 8)\n",
        "  assert (convert_to_five_categories(13) == 13)\n",
        "  assert (convert_to_five_categories(14) == 14)\n",
        "  print('Test complete. No Errors')\n",
        "test_converter()\n",
        "\n",
        "\n",
        "def convert_vector(v):\n",
        "  \"\"\"Reduces a vector of labels down to either 0 or one of the 5 desired categories \"\"\"\n",
        "  return [convert_to_five_categories(k) for k in v]\n",
        "\n",
        "def test_convert_vector():\n",
        "  assert((convert_vector([0, 0, 0, 0, 5, 0]) == [0, 0, 0, 0, 5, 0]))\n",
        "  assert((convert_vector([0, 0, 0, 0, 5, 6]) == [0, 0, 0, 0, 5, 6]))\n",
        "  assert((convert_vector([0, 0, 0, 0, 17, 0]) == [0, 0, 0, 0, 0, 0]))\n",
        "  assert((convert_vector([0, 0, 0, 0, 1, 0]) == [0, 0, 0, 0, 1, 0]))\n",
        "  print('Test complete. No Errors')\n",
        "test_convert_vector()\n",
        "\n",
        "def convert_datapoint(example):\n",
        "  example['ner_tags'] = convert_vector(example['ner_tags'])\n",
        "  return example\n"
      ],
      "metadata": {
        "id": "X3OTRNDjh-nI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_reduced = train_en.map(convert_datapoint)"
      ],
      "metadata": {
        "id": "gOX9Tx8AR6s5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_reduced = valid_en.map(convert_datapoint)\n",
        "test_reduced = test_en.map(convert_datapoint)"
      ],
      "metadata": {
        "id": "YMUjO90LWFtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
      ],
      "metadata": {
        "id": "Be8VtLdQp3ZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer(train_en[0]['tokens'], truncation=True, is_split_into_words=True, return_tensors=\"pt\", padding=True)"
      ],
      "metadata": {
        "id": "i1eF3d8OWuQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_align_labels(examples):\n",
        "  \"\"\"Re-align the NER tags after sub-word tokenization\n",
        "  CREDIT: https://huggingface.co/docs/transformers/tasks/token_classification\"\"\"\n",
        "  tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "  labels = []\n",
        "  for i, label in enumerate(examples[f\"ner_tags\"]):\n",
        "    word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
        "    previous_word_idx = None\n",
        "    label_ids = []\n",
        "    for word_idx in word_ids:  # Set the special tokens [CLS] and [SEP] to -100.\n",
        "      if word_idx is None:\n",
        "        label_ids.append(-100)\n",
        "      elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
        "        label_ids.append(label[word_idx])\n",
        "      else:\n",
        "        label_ids.append(-100)\n",
        "      previous_word_idx = word_idx\n",
        "    labels.append(label_ids)\n",
        "\n",
        "  tokenized_inputs[\"labels\"] = torch.tensor(labels)\n",
        "  return tokenized_inputs"
      ],
      "metadata": {
        "id": "RNeVyhParZOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenize_and_align_labels(train_en[0:2])"
      ],
      "metadata": {
        "id": "P0khr-PP7sLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_train =  train_en.map(tokenize_and_align_labels, batched=True)\n",
        "tokenized_valid = valid_en.map(tokenize_and_align_labels, batched=True)\n",
        "tokenized_test = test_en.map(tokenize_and_align_labels, batched=True)\n",
        "\n",
        "tokenized_train_reduced =  train_reduced.map(tokenize_and_align_labels, batched=True)\n",
        "tokenized_valid_reduced = valid_reduced.map(tokenize_and_align_labels, batched=True)\n",
        "# tokenized_test_reduced = test_reduced.map(tokenize_and_align_labels, batched=True)\n"
      ],
      "metadata": {
        "id": "SFFkrKK1w77M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "q14NZLpWxeJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seqeval = evaluate.load(\"seqeval\")"
      ],
      "metadata": {
        "id": "7bu6BUPvxoov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def compute_metrics(p):\n",
        "    \"\"\"First re-aligns the labels to the original state and then calculates metrics\n",
        "    \"\"\"\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    true_predictions = [\n",
        "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
        "    return {\n",
        "        \"precision\": results[\"overall_precision\"],\n",
        "        \"recall\": results[\"overall_recall\"],\n",
        "        \"f1\": results[\"overall_f1\"],\n",
        "        \"accuracy\": results[\"overall_accuracy\"],\n",
        "    }"
      ],
      "metadata": {
        "id": "3VNE1GGtx4SA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_list = list(label2id.keys())"
      ],
      "metadata": {
        "id": "5bIyHVBeyMJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the models using the traininer class"
      ],
      "metadata": {
        "id": "jw-YEBNZKwrT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    \"distilbert-base-uncased\", num_labels=31, id2label=id2label, label2id=label2id\n",
        ")\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"RISE_NER\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=2,\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    push_to_hub=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_valid,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.push_to_hub()"
      ],
      "metadata": {
        "id": "SMfodwJBybbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    \"distilbert-base-uncased\", num_labels=31, id2label=id2label, label2id=label2id\n",
        ")\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"RISE_NER_REDUCED\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=2,\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    push_to_hub=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_reduced,\n",
        "    eval_dataset=tokenized_valid_reduced,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.push_to_hub()\n"
      ],
      "metadata": {
        "id": "KPGgXjX-nf8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a repetition of the trainer code but loading the model from hugginface_hub. We do this to use the predict function of the trainer to evaluate on the test set."
      ],
      "metadata": {
        "id": "Xdky6ELuLw78"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForTokenClassification.from_pretrained(f\"{username}/RISE_NER\")\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"RISE_NER\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=2,\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    push_to_hub=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_valid,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "model.eval()\n",
        "\n",
        "predictions = trainer.predict(tokenized_test)"
      ],
      "metadata": {
        "id": "-aUgHmHyjSwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions"
      ],
      "metadata": {
        "id": "VR9BWaUygibo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = AutoModelForTokenClassification.from_pretrained(f\"{username}/RISE_NER_REDUCED\")\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"RISE_NER_REDUCED\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=2,\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    push_to_hub=True,\n",
        ")\n",
        "\n",
        "trainer2 = Trainer(\n",
        "    model=model2,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_valid,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "model2.eval()\n",
        "\n",
        "predictions = trainer2.predict(tokenized_test)"
      ],
      "metadata": {
        "id": "sAfLsIsqgrtc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions"
      ],
      "metadata": {
        "id": "iCFz0dykhd9H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}